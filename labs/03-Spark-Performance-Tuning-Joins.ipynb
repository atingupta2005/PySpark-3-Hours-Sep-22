{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a00c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf5f917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+-----------+\n",
      "|Add|    ID|Name|partitionID|\n",
      "+---+------+----+-----------+\n",
      "|USA|21.528|Jhon|          0|\n",
      "|USA|  3.69| Joe|          1|\n",
      "|IND|  2.48|Tina|          2|\n",
      "|USA| 22.22|Jhon|          3|\n",
      "|INA|  5.33| Joe|          3|\n",
      "+---+------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "data1 = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USA'},{'Name':'Joe','ID':5.33,'Add':'INA'}]\n",
    "a = sc.parallelize(data1)\n",
    "b = spark.createDataFrame(a)\n",
    "b=b.withColumn(\"partitionID\", spark_partition_id())\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "161a57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----------+\n",
      "|  Add|    ID|Name|partitionID|\n",
      "+-----+------+----+-----------+\n",
      "|India|21.528|Atin|          0|\n",
      "| USeA|  3.69| Joe|          1|\n",
      "|  IND|  2.48|Tina|          2|\n",
      "| USdA| 22.22|Jhon|          3|\n",
      "|  rsa|  5.33| Joe|          3|\n",
      "+-----+------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [{'Name':'Atin','ID':21.528,'Add':'India'},{'Name':'Joe','ID':3.69,'Add':'USeA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USdA'},{'Name':'Joe','ID':5.33,'Add':'rsa'}]\n",
    "c = sc.parallelize(data2)\n",
    "d = spark.createDataFrame(c)\n",
    "d=d.withColumn(\"partitionID\", spark_partition_id())\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe637ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (Add#389 = Add#370)\n",
      ":- LogicalRDD [Add#389, ID#390, Name#391], false\n",
      "+- LogicalRDD [Add#370, ID#371, Name#372], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Add: string, ID: double, Name: string, Add: string, ID: double, Name: string\n",
      "Join Inner, (Add#389 = Add#370)\n",
      ":- LogicalRDD [Add#389, ID#390, Name#391], false\n",
      "+- LogicalRDD [Add#370, ID#371, Name#372], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (Add#389 = Add#370)\n",
      ":- Filter isnotnull(Add#389)\n",
      ":  +- LogicalRDD [Add#389, ID#390, Name#391], false\n",
      "+- Filter isnotnull(Add#370)\n",
      "   +- LogicalRDD [Add#370, ID#371, Name#372], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [Add#389], [Add#370], Inner\n",
      "   :- Sort [Add#389 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(Add#389, 200), ENSURE_REQUIREMENTS, [id=#733]\n",
      "   :     +- Filter isnotnull(Add#389)\n",
      "   :        +- Scan ExistingRDD[Add#389,ID#390,Name#391]\n",
      "   +- Sort [Add#370 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(Add#370, 200), ENSURE_REQUIREMENTS, [id=#734]\n",
      "         +- Filter isnotnull(Add#370)\n",
      "            +- Scan ExistingRDD[Add#370,ID#371,Name#372]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = d.join(b,d.Add == b.Add)\n",
    "f.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c61552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+---+----+----+\n",
      "|Add|  ID|Name|Add|  ID|Name|\n",
      "+---+----+----+---+----+----+\n",
      "|IND|2.48|Tina|IND|2.48|Tina|\n",
      "+---+----+----+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "691d0260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (Add#389 = Add#370)\n",
      ":- LogicalRDD [Add#389, ID#390, Name#391], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [Add#370, ID#371, Name#372], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Add: string, ID: double, Name: string, Add: string, ID: double, Name: string\n",
      "Join Inner, (Add#389 = Add#370)\n",
      ":- LogicalRDD [Add#389, ID#390, Name#391], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [Add#370, ID#371, Name#372], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (Add#389 = Add#370), rightHint=(strategy=broadcast)\n",
      ":- Filter isnotnull(Add#389)\n",
      ":  +- LogicalRDD [Add#389, ID#390, Name#391], false\n",
      "+- Filter isnotnull(Add#370)\n",
      "   +- LogicalRDD [Add#370, ID#371, Name#372], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Add#389], [Add#370], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(Add#389)\n",
      "   :  +- Scan ExistingRDD[Add#389,ID#390,Name#391]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id=#873]\n",
      "      +- Filter isnotnull(Add#370)\n",
      "         +- Scan ExistingRDD[Add#370,ID#371,Name#372]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "f = d.join(broadcast(b),d.Add == b.Add)\n",
    "f.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "072fed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+---+----+----+\n",
      "|Add|  ID|Name|Add|  ID|Name|\n",
      "+---+----+----+---+----+----+\n",
      "|IND|2.48|Tina|IND|2.48|Tina|\n",
      "+---+----+----+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d37ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ceb6392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|     city|\n",
      "+----------+---------+\n",
      "|    andrea| medellin|\n",
      "|   rodolfo| medellin|\n",
      "|     abdul|bangalore|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF = (\n",
    "  (\"andrea\", \"medellin\"),\n",
    "  (\"rodolfo\", \"medellin\"),\n",
    "  (\"abdul\", \"bangalore\")\n",
    ")\n",
    "\n",
    "peopleDF_a = sc.parallelize(peopleDF)\n",
    "peopleDF_b = spark.createDataFrame(peopleDF_a, [\"first_name\", \"city\"])\n",
    "\n",
    "peopleDF_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8a25f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+\n",
      "|     city| country|population|\n",
      "+---------+--------+----------+\n",
      "| medellin|colombia|       2.5|\n",
      "|bangalore|   india|      12.3|\n",
      "+---------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citiesDF = (\n",
    "  (\"medellin\", \"colombia\", 2.5),\n",
    "  (\"bangalore\", \"india\", 12.3)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "citiesDF_a = sc.parallelize(citiesDF)\n",
    "citiesDF_b = spark.createDataFrame(citiesDF_a, (\"city\", \"country\", \"population\"))\n",
    "\n",
    "citiesDF_b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53c34031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (city#319 = city#331)\n",
      ":- LogicalRDD [first_name#318, city#319], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [city#331, country#332, population#333], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, city: string, city: string, country: string, population: double\n",
      "Join Inner, (city#319 = city#331)\n",
      ":- LogicalRDD [first_name#318, city#319], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [city#331, country#332, population#333], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (city#319 = city#331), rightHint=(strategy=broadcast)\n",
      ":- Filter isnotnull(city#319)\n",
      ":  +- LogicalRDD [first_name#318, city#319], false\n",
      "+- Filter isnotnull(city#331)\n",
      "   +- LogicalRDD [city#331, country#332, population#333], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [city#319], [city#331], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(city#319)\n",
      "   :  +- Scan ExistingRDD[first_name#318,city#319]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id=#660]\n",
      "      +- Filter isnotnull(city#331)\n",
      "         +- Scan ExistingRDD[city#331,country#332,population#333]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  broadcast(citiesDF_b),\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2504100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (city#319 = city#331)\n",
      ":- LogicalRDD [first_name#318, city#319], false\n",
      "+- LogicalRDD [city#331, country#332, population#333], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "first_name: string, city: string, city: string, country: string, population: double\n",
      "Join Inner, (city#319 = city#331)\n",
      ":- LogicalRDD [first_name#318, city#319], false\n",
      "+- LogicalRDD [city#331, country#332, population#333], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (city#319 = city#331)\n",
      ":- Filter isnotnull(city#319)\n",
      ":  +- LogicalRDD [first_name#318, city#319], false\n",
      "+- Filter isnotnull(city#331)\n",
      "   +- LogicalRDD [city#331, country#332, population#333], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [city#319], [city#331], Inner\n",
      "   :- Sort [city#319 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(city#319, 200), ENSURE_REQUIREMENTS, [id=#683]\n",
      "   :     +- Filter isnotnull(city#319)\n",
      "   :        +- Scan ExistingRDD[first_name#318,city#319]\n",
      "   +- Sort [city#331 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(city#331, 200), ENSURE_REQUIREMENTS, [id=#684]\n",
      "         +- Filter isnotnull(city#331)\n",
      "            +- Scan ExistingRDD[city#331,country#332,population#333]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF_b_j = peopleDF_b.join(\n",
    "  citiesDF_b,\n",
    "  peopleDF_b.city == citiesDF_b.city\n",
    ")\n",
    "\n",
    "peopleDF_b_j.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29059774",
   "metadata": {},
   "source": [
    "#### Notice how the parsed, analyzed, and optimized logical plans all contain ResolvedHint isBroadcastable=true because the broadcast() function was used. This hint isn’t included when the broadcast() function isn’t used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851a761",
   "metadata": {},
   "source": [
    "## Automatic Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc0a184",
   "metadata": {},
   "source": [
    "- In many cases, Spark can automatically detect whether to use a broadcast join or not, depending on the size of the data. If Spark can detect that one of the joined DataFrames is small (10 MB by default), Spark will automatically broadcast it for us. The code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec1101f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(id))\n",
      ":- Range (1, 10000, step=1, splits=Some(4))\n",
      "+- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [id#276L]\n",
      "+- Join Inner, (id#276L = id#274L)\n",
      "   :- Range (1, 10000, step=1, splits=Some(4))\n",
      "   +- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#276L]\n",
      "+- Join Inner, (id#276L = id#274L)\n",
      "   :- Range (1, 10000, step=1, splits=Some(4))\n",
      "   +- Range (1, 100000000, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#276L]\n",
      "   +- BroadcastHashJoin [id#276L], [id#274L], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#585]\n",
      "      :  +- Range (1, 10000, step=1, splits=4)\n",
      "      +- Range (1, 100000000, step=1, splits=4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigTable = spark.range(1, 100000000)\n",
    "smallTable = spark.range(1, 10000) # size estimated by Spark - auto-broadcast\n",
    "joinedNumbers = smallTable.join(bigTable, \"id\")\n",
    "joinedNumbers.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9248fc0",
   "metadata": {},
   "source": [
    "- However, in the previous case, Spark did not detect that the small table could be broadcast. \n",
    "- The reason is that Spark will not determine the size of a local collection because it might be big, and evaluating its size may be an O(N) operation, which can defeat the purpose before any computation is made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1124ba",
   "metadata": {},
   "source": [
    "- Spark will perform auto-detection when\n",
    "    - it constructs a DataFrame from scratch, e.g. spark.range\n",
    "    - it reads from files with schema and/or size information, e.g. Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b5c7b",
   "metadata": {},
   "source": [
    "## Configuring Broadcast Join Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e79a50",
   "metadata": {},
   "source": [
    "- The threshold for automatic broadcast join detection can be tuned or disabled.\n",
    "- The configuration is spark.sql.autoBroadcastJoinThreshold, and the value is taken in bytes. If you want to configure it to another number, we can set it in the SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a98b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211cc1d",
   "metadata": {},
   "source": [
    "- or deactivate it altogether by setting the value to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bad2c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
