{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2113c0d6-8f88-4e28-bc1c-7bc58f2e2d76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark Performance Tuning & Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ff45988-ffc0-444f-8eac-13094e268cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Use DataFrame/Dataset over RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a42f7d33-d04d-465c-bb2f-b410af2df05b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- For Spark jobs, prefer using Dataset/DataFrame over RDD as Dataset and DataFrame’s includes several optimization modules to improve the performance of the Spark workloads\n",
    "- In PySpark use, DataFrame over RDD as Dataset’s are not supported in PySpark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "494304af-fd6b-4136-9b7f-effb739e971c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Spark RDD is a building block of Spark programming, even when we use DataFrame/Dataset, Spark internally uses RDD to execute operations/queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79385569-68d3-497a-b386-69355af320e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why RDD is slow?\n",
    "- Using RDD directly leads to performance issues as Spark doesn’t know how to apply the optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "237e9cff-6e24-4640-9214-9081c05bf871",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Use coalesce() over repartition()\n",
    "- When you want to reduce the number of partitions prefer using coalesce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a5bbb04d-7540-47a0-bee6-c8d02396dc6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Use Parquet data format’s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b33d367f-d43d-4b6a-b847-8d3c9866ac3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Avoid UDF’s (User Defined Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "930874af-6013-4b7e-a233-260c74d111e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Try to avoid Spark/PySpark UDF’s at any cost and use when existing Spark built-in functions are not available for use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8251f89-bb37-41d3-8137-6bcb68f097b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Persisting data\n",
    "- Using persist() method, Spark provides an optimization mechanism to store the intermediate computation of a Spark DataFrame so they can be reused in subsequent actions.\n",
    "- Well, suppose you have written a few transformations to be performed on an RDD. \n",
    "- Now each time you call an action on the RDD, Spark recomputes the RDD and all its dependencies. \n",
    "- This can turn out to be quite expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7d583949-b3a4-4bb9-acac-3e497be6d107",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "l1 = [1, 2, 3, 4]\n",
    "\n",
    "rdd1 = sc.parallelize(l1)\n",
    "rdd2 = rdd1.map(lambda x: x*x)\n",
    "rdd3 = rdd2.map(lambda x: x+2)\n",
    "\n",
    "# When I call count(), all the transformations are performed and it takes 0.1 s to complete the task.\n",
    "print(rdd3.count())\n",
    "\n",
    "# When I call collect(), again all the transformations are called and it still takes me 0.1 s to complete the task.\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c603310-89af-42d1-8ea8-4342ed919928",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- So how do we get out of this vicious cycle? Persist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1373a910-8e1e-479e-8c3f-667d2fd8683f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# By default cached to memory and disk\n",
    "rdd3.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# before rdd is persisted (It will be persisted on first action as below)\n",
    "print(rdd3.count())\n",
    "\n",
    "\n",
    "# after rdd is persisted (After the previous action is executed)\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a53bd92f-c8a8-4f57-b219-4c243d6125c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- In our previous code, all we have to do is persist in the final RDD.\n",
    "- This way when we first call an action on the RDD, the final data generated will be stored in the cluster.\n",
    "- Now, any subsequent use of action on the same RDD would be much faster as we had already stored the previous result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "19f56e80-9db8-4ee5-88fd-beaffff7913f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reduce expensive Shuffle operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d1e5ba8-5d28-4243-84ca-1a9e7d035e3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Spark shuffling triggers when we perform certain transformation operations like gropByKey(), reducebyKey(), join()\n",
    "- Spark Shuffle is an expensive operation since it involves the following\n",
    "  - Disk I/O\n",
    "  - Involves data serialization and deserialization\n",
    "  - Network I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9b9f2fc-8fa7-4876-8cd2-77c52ca1b651",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- DataFrame increases the partition number to 200 automatically when Spark operation performs data shuffling (join(), aggregation functions).\n",
    "- You can change this default shuffle partition value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a788bbe6-89d9-4cfb-81ca-15790ebe2004",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e8e7f89d-c601-44f0-9208-fea3f82541e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Don’t Collect Data\n",
    "- When we call the collect action, the result is returned to the driver node\n",
    "- If you are working with huge amounts of data, then the driver node might easily run out of memory.\n",
    "- One great way to escape is by using the take() action. It scans the first partition it finds and returns the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ecc48def-af9b-45ba-aad0-263320f89fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Broadcast Large Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dd2e525-4450-4ad8-b477-1ae25528a434",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Just like accumulators, Spark has another shared variable called the Broadcast variable\n",
    "- They are only used for reading purposes that get cached in all the worker nodes in the cluster\n",
    "- This comes in handy when you have to send a large look-up table to all nodes.\n",
    "\n",
    "- Assume a file containing data containing the shorthand code for countries (like IND for India) with other kinds of information\n",
    "- You have to transform these codes to the country name\n",
    "- This is where Broadcast variables come in handy using which we can cache the lookup tables in the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1deef94-99a3-4142-b0b3-06a0ec021e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lookup\n",
    "country = {\"IND\":\"India\",\"USA\":\"United States of America\",\"SA\":\"South Africa\"}\n",
    "\n",
    "# broadcast\n",
    "broadcaster = sc.broadcast(country)\n",
    "\n",
    "# data\n",
    "userData = [(\"Johnny\",\"USA\"),(\"Faf\",\"SA\"),(\"Sachin\",\"IND\")]\n",
    "\n",
    "# create rdd\n",
    "rdd_data = sc.parallelize(userData)\n",
    "\n",
    "# use broadcast variable\n",
    "def convert(code):\n",
    "    return broadcaster.value[code]\n",
    "\n",
    "# transformation\n",
    "output = rdd_data.map(lambda x: (x[0], convert(x[1])))\n",
    "\n",
    "# action\n",
    "output.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ec9c87b-4af5-4d07-96a1-20c282410a90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Be shrewd with Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2fad54c-b253-44a6-930b-61756091d2ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- One of the cornerstones of Spark is its ability to process data in a parallel fashion. \n",
    "- Spark splits data into several partitions, each containing some subset of the complete data. \n",
    "- For example, if a dataframe contains 10,000 rows and there are 10 partitions, then each partition will have 1000 rows.\n",
    "- When Spark runs a task, it is run on a single partition in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b75e60b8-36e7-430f-ba9f-58572a90a6f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Choose too few partitions, you have a number of resources sitting idle.\n",
    "- Choose too many partitions, you have a large number of small partitions shuffling data frequently, which can become highly inefficient.\n",
    "- So what’s the right number?\n",
    "- According to Spark, 128 MB is the maximum number of size you should pack into a single partition.\n",
    "- So, if we have 128000 MB of data, we should have 1000 partitions. But this number is not rigid as we will see in the next tip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f31920d3-fb43-4381-bf81-2fb8b419a4c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Monitoring of Job Stages\n",
    "- Most of the developers write and execute the code, but monitoring of Job tasks is essential. This monitoring is best achieved by managing DAG and reducing the stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60411bea-8502-4888-9716-fa8d5bb8b05c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Use Predicate Pushdown\n",
    "- Predicate push down is another feature of Spark and Parquet that can improve query performance by reducing the amount of data read from Parquet files.\n",
    "- Push down means the filters are pushed to the source as opposed to being brought into Spark"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Performance-PySpark",
   "notebookOrigID": 4326754976675683,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
